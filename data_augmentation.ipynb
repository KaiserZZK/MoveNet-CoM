{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb92b55",
   "metadata": {},
   "source": [
    "# DATASET\n",
    "\n",
    "### I. Our custom dataset format\n",
    "\n",
    "All data should be converted to our custom dataset format before being used for training. Our format has this folder structure:\n",
    "```\n",
    "dataset_name/\n",
    "    images/\n",
    "    train.json\n",
    "    val.json\n",
    "    test.json\n",
    "```\n",
    "\n",
    "- `images` is a folder containing image files.\n",
    "- `train.json`, `val.json`, `test.json` are annotation files. Here are an example of labels in these files:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"image\": \"001.png\",\n",
    "        \"points\": [[280, 540], [315, 468], [356, 354], [354, 243], [471, 331], [514, 440], [546, 540]],\n",
    "        \"visibility\": [1, 1, 1, 1, 0, 0, 1]\n",
    "    }\n",
    "    {\n",
    "        \"image\": \"002.png\",\n",
    "        \"points\": [[269, 529], [289, 465], [305, 410], [310, 309], [455, 358], [542, 429], [560, 542]],\n",
    "        \"visibility\": [1, 0, 0, 1, 1, 1, 1]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### II. LSP and LSPET\n",
    "\n",
    "- Link to LSP dataset: <https://sam.johnson.io/research/lsp.html>.\n",
    "- Link to LSPET dataset: <https://sam.johnson.io/research/lspet.html>.\n",
    "\n",
    "#### 1. Convert annotation to JSON format\n",
    "\n",
    "- The annotation contains x and y locations and a binary value indicating the visbility of joints.\n",
    "- Use `tools/lsp_data_to_json.py` to convert LSP and LSPET annotation files to json format:\n",
    "- **NOTE:** We removed 6061 images from LSPET dataset due to missing points.\n",
    "\n",
    "```\n",
    "python tools/lsp_data_to_json.py --image_folder=data/lsp_dataset/images --input_file data/lsp_dataset/joints.mat --output_file data/lsp_dataset/labels.json\n",
    "python tools/lsp_data_to_json.py --image_folder=data/lspet_dataset/images --input_file data/lspet_dataset/joints.mat --output_file data/lspet_dataset/labels.json\n",
    "```\n",
    "\n",
    "#### 2. Merge 2 dataset and divide into subsets\n",
    "\n",
    "+ Training: 3739 from LSPET and 1800 from LSP.\n",
    "+ Validation: 100 from LSPET and 100 from LSP.\n",
    "+ Test: 100 from LSPET and 100 from LSP.\n",
    "\n",
    "Please update paths to LSP and LSPET in `tools/split_lsp_lspet.py` and run:\n",
    "\n",
    "```\n",
    "python tools/split_lsp_lspet.py\n",
    "```\n",
    "\n",
    "\n",
    "### III. MPII Humanpose\n",
    "\n",
    "- We only use images with numOtherPeople = 0. The original dataset are divided into 3 subsets:\n",
    "\n",
    "+ Training: 9503 images.\n",
    "+ Validation: 1000 images.\n",
    "+ Test: 1000 images.\n",
    "\n",
    "\n",
    "### IV. PushUp dataset\n",
    "\n",
    "We have push-up 420 videos, divided in 3 sets:\n",
    "\n",
    "+ Training: 8837 images from 317 videos.\n",
    "+ Validation: 1189 images from 41 videos.\n",
    "+ Test: 1013 images from 62 videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_MoveNet_test1",
   "language": "python",
   "name": "venv_movenet_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
